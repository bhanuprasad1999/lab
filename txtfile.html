<pre>
    #Program-1
    def aStarAlgo(start_node, stop_node):
    open_set=set(start_node)
    closed_set = set()
    g = {}
    parents = {}
    g[start_node]= 0
    parents[start_node] = start_node
    while len(open_set) > 0:
    n = None
    for v in open_set:
    if n == None or g[v] + heuristic(v) < g[n]+ heuristic(n):
    n = v
    if n == stop_node or Graph_nodes[n] == None:
    pass
    else:
    for (m, weight) in get_neighbors(n):
    if m not in open_set and m not in closed_set:
    open_set.add(m)
    parents[m] = n
    g[m] = g[n] + weight
    else:
    if g[m] > g[n] +weight:
    g[m] = g[n] + weight
    parents[m] = n
    if m in closed_set:
    closed_set.remove(m)
    open_set.add(m)
    if n == None:
    print("Path doesn't Exist")
    return None
    if n == stop_node:
    path= []
    while parents[n] != n:
    path.append(n)
    n = parents[n]
path.append(start_node)
path.reverse()
print('Path found: {} '.format(path))
return path
open_set.remove(n)
closed_set.add(n)
print("Path--- doesn't exist")
return None
def get_neighbors(v):
if v in Graph_nodes:
return Graph_nodes[v]
else:
return None
def heuristic(n):
H_dist = {
'A':10,
'B':8,
'C':5,
'D':7,
'E':3,
'F':6,
'G':5,
'H':3,
'I':1,
'J':0
}
return H_dist[n]
Graph_nodes = {
    'A':[('B',6),('F',3)],
    'B':[('C',3),('D',2)],
    'C':[('D',1),('E',5)],
    'D':[('C',1),('E',8)],
    'E':[('I',5),('J',5)],
    'F':[('G',1),('H',7)],
    'G':[('I',3)],
    'H':[('I',2)],
    'I':[('E',5),('J',3)]
    }
    aStarAlgo('A', 'J')


# program-2
class Graph:
def __init__(self, graph, heuristicNodeList,startnode):
self.graph = graph
self.H = heuristicNodeList
self.start = startnode
self.parent = {}
self.status = {}
self.solutionGraph={}
def applyAOStar(self):
self.aoStar(self.start,False)
def getNeighbors(self,v):
return self.graph.get(v,'')
def getStatus(self,v):
return self.status.get(v,0)
def setStatus(self,v,val):
self.status[v]=val
def getHeuristicNodeValue(self,n):
return self.H.get(n,0)
def setHeuristicNodeValue(self,n,value):
self.H[n]=value
def printSolution(self):
print("FOR GRAPH SOLUTION, TRAVERSE THE GRAPH FROM THE START
NODE:", self.start)
print("------------------------------------------------------------")
print(self.solutionGraph)
print("------------------------------------------------------------")
def computeMinimumCostChildNodes(self,v):
minimumCost=0
costToChildNodeListDict={}
costToChildNodeListDict[minimumCost]=[]
flag=True
for nodeInfoTupleList in self.getNeighbors(v):
cost=0
nodeList=[]
for c, weight in nodeInfoTupleList:
cost=cost+self.getHeuristicNodeValue(c)+weight
nodeList.append(c)
if flag == True:
minimumCost=cost
costToChildNodeListDict[minimumCost]=nodeList
flag=False
else:
if minimumCost > cost:
minimumCost = cost
costToChildNodeListDict[minimumCost]=nodeList
return minimumCost,costToChildNodeListDict[minimumCost]
def aoStar(self,v,backTracking):
print("HEURISTIC VALUES:",self.H)
print("SOLUTION GRAPH:",self.solutionGraph)
print("PROCESSING NODE:", v)
print("-----------------------------------")
if self.getStatus(v) >= 0:
minimumCost,childNodeList,=self.computeMinimumCostChildNodes(v)
self.setHeuristicNodeValue(v,minimumCost)
self.setStatus(v,len(childNodeList))
solved=True
for childNode in childNodeList:
self.parent[childNode]=v
if self.getStatus(childNode) != -1:
solved= solved & False
if solved == True:
self.setStatus(v,-1)
self.solutionGraph[v]=childNodeList
if v != self.start:
self.aoStar(self.parent[v],True)
if backTracking == False:
for childNode in childNodeList:
self.setStatus(childNode,0)
self.aoStar(childNode,False)
h1 ={'A':1, 'B':6, 'C':12, 'D':10, 'E':4, 'F':4, 'G':5, 'H':7}
graph1 = {
'A' : [[('B',1),('C',1)],[('D',1)]],
'B' : [[('G',1)],[('H',1)]],
'D' : [[('E',1),('F',1)]]
}
G1=Graph(graph1,h1,'A')
G1.applyAOStar()
G1.printSolution()

# program-3
import random
import csv
def g_0(n):
return ("?",)*n
def s_0(n):
return ('Φ',)*n
def more_general(h1, h2):
more_general_parts = []
for x,y in zip(h1,h2):
mg = x =="?" or (x!= 'Φ' and (x == y or y == 'Φ'))
more_general_parts.append(mg)
return all(more_general_parts)
def fulfills(example , hypothesis):
return more_general(hypothesis, example)
def min_generalizations(h ,x):
h_new = list(h)
for i in range(len(h)):
if not fulfills(x[i:i+1], h[i:i+1]):
h_new[i] = '?' if h[i] != 'Φ' else x[i]
return [tuple(h_new)]
def min_specializations(h, domains, x):
results = []
for i in range(len(h)):
if h[i] == "?":
for val in domains[i]:
if x[i] != val:
h_new = h[:i] + (val,) + h[i+1:]
results.append(h_new)
elif h[i] != 'Φ':
h_new = h[:i] + ('Φ',) + h[i+1:]
results.append(h_new)
return results
with open('weather.csv') as csvFile:
examples = [tuple(line) for line in csv.reader(csvFile)]
def get_domains(examples):
d = [set() for i in examples[0]]
for x in examples:
for i, xi in enumerate(x):
d[i].add(xi)
return [list(sorted(x)) for x in d]
get_domains(examples)
def candidate_elimination(examples):
domains = get_domains(examples)[:-1]
G = set([g_0(len(domains))])
S = set([s_0(len(domains))])
i=0
print("\n G[{0}]:".format(i), G)
print("\n S[{0}]:".format(i), S)
for xcx in examples:
i +=1
x, cx = xcx[:-1], xcx[-1] # Splitting data into attributes
and decisions
if cx == 'Yes':
G = {g for g in G if fulfills(x, g)}
S = generalize_S(x, G, S)
else:
S = {s for s in S if not fulfills(x, S)}
G = specialize_G(x, domains, G, S)
print("\n G[{0}]:".format(i), G)
print("\n S[{0}]:".format(i), S)
return
def generalize_S(x, G, S):
S_prev = list(S)
for s in S_prev:
if s not in S:
continue
if not fulfills(x, s):
S.remove(s)
Splus = min_generalizations(s, x)
# Keep only generalizations that have a counterpart in G
S.update([h for h in Splus if any([more_general(g ,h)
for g in G])])
# Remove the hypotheses less specific than any other in
S
S.difference_update([h for h in S if any([more_general(h,
h1) for h1 in S if h!= h1])])
return S
def specialize_G(x, domains, G, S):
G_prev = list(G)
for g in G_prev:
if g not in G:
continue
if fulfills(x, g):
G.remove(g)
Gminus = min_specializations(g, domains, x)
# Keep only specializations that have a counterpart in S
G.update([h for h in Gminus if any([more_general(h, s)
for s in S])])
# Remove the hypotheses less general than any other in G
G.difference_update([h for h in G if
any([more_general(g1, h) for g1 in G if h != g1])])
return G
candidate_elimination(examples)

# program-4
from collections import Counter
import math
from pprint import pprint
import pandas as pd
import warnings
from pandas import DataFrame

# To remove warning SettingWithCopy warning
from pandas.core.common import SettingWithCopyWarning
warnings.simplefilter(action="ignore",
                      category=SettingWithCopyWarning)
df_tennis = pd.read_csv('tennis.csv')
# Function to calculate the entropy of probability of observations
# -p*log2*p
def entropy(probs):


return sum([-prob*math.log(prob, 2) for prob in probs])
# Function to calculate the entropy of the given Data Sets/List with
respect to target attributes
def entropy_of_list(a_list):


cnt = Counter(x for x in a_list)  # Counter calculates the
proportion of class
num_instances = len(a_list)*1.0  # = 14
probs = [x / num_instances for x in cnt.values()]  # x means no
of YES/NO
return entropy(probs)  # Call Entropy :
12
total_entropy = entropy_of_list(df_tennis['PlayTennis'])
print("\n Total Entropy of PlayTennis Data Set:", total_entropy,
      "\n\n")


def information_gain(df, split_attribute_name, target_attribute_name,
                     trace=0):


'''
Takes a DataFrame of attributes, and quantifies the entropy of a
target
attribute after performing a split along the values of another
attribute.
'''
# Split Data by Possible Values of Attribute:
df_split = df.groupby(split_attribute_name)
# Calculate Entropy for Target Attribute, as well as
# Proportion of Obs in Each Data-Split
nobs = len(df.index) * 1.0
df_agg_ent = df_split.agg({target_attribute_name:
                           [entropy_of_list, lambda x: len(x)/nobs]})[target_attribute_name]
df_agg_ent.columns = ['Entropy', 'PropObservations']
if trace:  # helps understand what func. is doing:
print(df_agg_ent)
# Calculate Information Gain:
new_entropy = sum(df_agg_ent['Entropy'] *
                  df_agg_ent['PropObservations'])
old_entropy = entropy_of_list(df[target_attribute_name])
return old_entropy - new_entropy
print('\n Info-gain for Outlook is :'+str(
    information_gain(df_tennis, 'Outlook', 'PlayTennis')), "\n")
print('\n Info-gain for Humidity is: ' + str(
    information_gain(df_tennis, 'Humidity', 'PlayTennis')), "\n")
print('\n Info-gain for Wind is:' + str(information_gain(df_tennis,
                                                         'Wind', 'PlayTennis')), "\n")
print('\n Info-gain for Temperature is:' + str(
    information_gain(df_tennis, 'Temperature', 'PlayTennis')), "\n")
13
def id3(df, target_attribute_name, attribute_names,
        default_class=None):


    # Tally target attribute:
cnt = Counter(x for x in df[target_attribute_name])  # class of YES
/NO
# First check: Is this split of the dataset homogeneous?
if len(cnt) == 1:
return next(iter(cnt))  # next input data set, or raises
StopIteration when EOF is hit.
# Second check: Is this split of the dataset empty?
# if yes, return a default value
elif df.empty or (not attribute_names):
return default_class  # Return None for Empty Data Set
# Otherwise: This dataset is ready to be divided up!
else:
    # Get Default Value for next recursive call of this function:
default_class = max(cnt.keys())  # No of YES and NO Class
# Compute the Information Gain of the attributes:
gainz = [information_gain(df, attr, target_attribute_name)
         for attr in attribute_names]
index_of_max = gainz.index(max(gainz))  # Index of Best
Attribute
# Choose Best Attribute to split on:
best_attr = attribute_names[index_of_max]
# Create an empty tree, to be populated in a moment
tree = {best_attr: {}}  # Initiate the tree with best attribute
as a node
remaining_attribute_names = [i for i in attribute_names if i
                             != best_attr]
# Split dataset
# On each split, recursively call this algorithm.
14
# populate the empty tree with subtrees, which
# are the result of the recursive call
for attr_val, data_subset in df.groupby(best_attr):
subtree = id3(data_subset,
              target_attribute_name,
              remaining_attribute_names,
              default_class)
tree[best_attr][attr_val] = subtree
return tree
# Get Predictor Names (all but 'class')
attribute_names = list(df_tennis.columns)
attribute_names.remove('PlayTennis')  # Remove the class attribute
# Run Algorithm:
tree = id3(df_tennis, 'PlayTennis', attribute_names)
print("\n\nThe Resultant Decision Tree is :\n")
pprint(tree)
attribute = next(iter(tree))
def classify(instance, tree, default=None):  # Instance of Play Tennis


with Predicted
attribute = next(iter(tree))  # Outlook/Humidity/Wind
if instance[attribute] in tree[attribute].keys():  # Value of the
attributes in set of Tree keys
result = tree[attribute][instance[attribute]]
if isinstance(result, dict):  # this is a tree, delve deeper
return classify(instance, result)
else:
return result  # this is a label
else:
return default
15
df_tennis['actual'] = df_tennis.apply(classify, axis=1,
                                      args=(tree, 'No'))
# classify func allows for a default arg: when tree doesn't have
answer for a particular
# combination of attribute-values, we can use 'no' as the default
guess
df_tennis[['PlayTennis', 'actual']]
training_data = df_tennis.iloc[1:-4]  # all but last four instances
test_data = df_tennis.iloc[-4:]  # just the last four
train_tree = id3(training_data, 'PlayTennis', attribute_names)
test_data['predicted'] = test_data.apply(
    # <---- test_data source
    classify,
    axis=1,
    args=(train_tree, 'Yes'))
<---- train_data tree
print("\n\n", test_data)
print('\n\n Accuracy is : ' + str(
    sum(test_data['PlayTennis'] == test_data['predicted']) /
    (1.0*len(test_data.index))))

    #program-5
    import numpy as np
X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)
y = np.array(([92], [86], [89]), dtype=float)
X = X/np.amax(X,axis=0) # maximum of X array longitudinally
y = y/100
#Sigmoid Function
def sigmoid (x):
return 1/(1 + np.exp(-x))
#Derivative of Sigmoid Function
def derivatives_sigmoid(x):
return x * (1 - x)
#Variable initialization
epoch=5000 #Setting training iterations
lr=0.1 #Setting learning rate
inputlayer_neurons = 2 #number of features in data set
hiddenlayer_neurons = 3 #number of hidden layers neurons
output_neurons = 1 #number of neurons at output layer
#weight and bias initialization
wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))
bh=np.random.uniform(size=(1,hiddenlayer_neurons))
wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))
bout=np.random.uniform(size=(1,output_neurons))
#draws a random range of numbers uniformly of dim x*y
for i in range(epoch):
#Forward Propagation
hinp1=np.dot(X,wh)
hinp=hinp1 + bh
hlayer_act = sigmoid(hinp)
18
outinp1=np.dot(hlayer_act,wout)
outinp= outinp1+ bout
output = sigmoid(outinp)
#Backpropagation
EO = y-output
outgrad = derivatives_sigmoid(output)
d_output = EO* outgrad
EH = d_output.dot(wout.T)
#how much hidden layer wts contributed to error
hiddengrad = derivatives_sigmoid(hlayer_act)
d_hiddenlayer = EH * hiddengrad
# dot product of nextlayer error and current layer op
wout += hlayer_act.T.dot(d_output) *lr
wh += X.T.dot(d_hiddenlayer) *lr
print("Input: \n" + str(X))
print("Actual Output: \n" + str(y))
print("Predicted Output: \n" ,output)

#Program-6
print("Naive Bayes Classifier for Concept Learning \n")
import csv
import random
import math
import operator
def safe_div(x, y):
if y==0:
return 0
return x/y
def loadCsv(filename):
lines = csv.reader(open(filename))
dataset = list(lines)
# print(dataset)
for i in range(len(dataset)):
# print(dataset[i])
dataset[i] = [float(x) for x in dataset[i]]
return dataset
def splitDataset(dataset, splitRatio):
trainSize = int(len(dataset) * splitRatio)
trainSet = []
copy = list(dataset)
i=0
while len(trainSet) < trainSize:
trainSet.append(copy.pop(i))
return [trainSet, copy]
20
def separateByClass(dataset):
separated = {}
for i in range(len(dataset)):
vector = dataset[i]
if (vector[-1] not in separated):
separated[vector[-1]] = []
separated[vector[-1]].append(vector)
return separated
def mean(numbers):
return safe_div(sum(numbers), float(len(numbers)))
def stdev(numbers):
avg = mean(numbers)
variance = safe_div(sum([pow(x-avg, 2) for x in numbers]),
float(len(numbers)-1))
return math.sqrt(variance)
def summarize(dataset):
summaries = [(mean(attribute), stdev(attribute)) for attribute in
zip(*dataset)]
del summaries[-1]
return summaries
def summarizeByClass(dataset):
separated = separateByClass(dataset)
summaries = {}
for classValue, instances in separated.items():
summaries[classValue] = summarize(instances)
print("Summarize Attribute By Class")
print(summaries)
print(" ")
return summaries
21
def calculateProbability(x, mean, stdev):
exponent = math.exp(-safe_div(math.pow(x-mean, 2),
(2*math.pow(stdev, 2))))
final = safe_div(1, (math.sqrt(2*math.pi) * stdev)) * exponent
return final
def calculateClassProbabilities(summaries, inputVector):
probabilities = {}
for classValue, classSummaries in summaries.items():
probabilities[classValue] = 1
for i in range(len(classSummaries)):
mean, stdev = classSummaries[i]
x = inputVector[i]
probabilities[classValue] *= calculateProbability(x, mean,
stdev)
return probabilities
def predict(summaries, inputVector):
probabilities = calculateClassProbabilities(summaries,
inputVector)
bestLabel, bestProb = None, -1
for classValue, probability in probabilities.items():
if bestLabel is None or probability > bestProb:
bestProb = probability
bestLabel = classValue
return bestLabel
def getPredictions(summaries, testSet):
predictions = []
for i in range(len(testSet)):
result = predict(summaries, testSet[i])
predictions.append(result)
return predictions
22
def getAccuracy(testSet, predictions):
correct = 0
for i in range(len(testSet)):
if testSet[i][-1] == predictions[i]:
correct += 1
accuracy = safe_div(correct, float(len(testSet))) * 100.0
return accuracy
def main():
filename = 'tennis_NB.csv'
splitRatio = 0.9
dataset = loadCsv(filename)
trainingSet, testSet = splitDataset(dataset, splitRatio)
print("Split {0} rows into".format(len(dataset)))
print("Number of Training Data: "+ (repr(len(trainingSet))))
print("Number of Test Data: "+ (repr(len(testSet))))
print("\nThe Values assumed for the concept learing attiributes
are: \n")
print("OUTLOOK=> Sunny=1 Overcast=2 Rain=3\nTEMPERATURE=> Hot=1
Mild=2 Cool=3\nHUMIDITY=> High=1 Normal=2\nWIND=> Weak=1 Strong=2")
print("TARGET CONCEPT: PLAY TENNIS=> Yes=10 No=5")
print('\n')
summaries = summarizeByClass(trainingSet)
predictions = getPredictions(summaries, testSet)
actual = []
for i in range(len(testSet)):
vector = testSet[i]
actual.append(vector[-1])
print('Actual values: {0}'.format(actual))
print('Predictions: {0}'.format(predictions))
accuracy = getAccuracy(testSet, predictions)
print('Accuracy: {0}%'.format(accuracy))
main()

#program-7
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.cluster import KMeans
import sklearn.metrics as sm
import pandas as pd
import numpy as np
l1 = [0,1,2]
def rename(s):
l2 = []
for i in s:
if i not in l2:
l2.append(i)
for i in range(len(s)):
pos = l2.index(s[i])
s[i] = l1[pos]
return s
# import some data to play with
iris = datasets.load_iris()
print("\n IRIS FEATURES :\n",iris.feature_names)
print("\n IRIS TARGET NAMES:\n",iris.target_names)
# Store the inputs as a Pandas Dataframe and set the column names
X = pd.DataFrame(iris.data)
#print(X)
X.columns =
['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']
30
y = pd.DataFrame(iris.target)
y.columns = ['Targets']
# Set the size of the plot
plt.figure(figsize=(14,7))
# Create a colormap
colormap = np.array(['red', 'lime', 'black'])
# Plot Sepal
plt.subplot(1,2,1)
plt.scatter(X.Sepal_Length,X.Sepal_Width, c=colormap[y.Targets],
s=40)
plt.title('Sepal')
plt.subplot(1,2,2)
plt.scatter(X.Petal_Length,X.Petal_Width, c=colormap[y.Targets],
s=40)
plt.title('Petal')
plt.show()
print("Actual Target is:\n", iris.target)
# K Means Cluster
model = KMeans(n_clusters=3)
model.fit(X)
# Set the size of the plot
plt.figure(figsize=(14,7))
# Create a colormap
colormap = np.array(['red', 'lime', 'black'])
# Plot the Original Classifications
plt.subplot(1,2,1)
plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[y.Targets],
s=40)
plt.title('Real Classification')
# Plot the Models Classifications
31
plt.subplot(1,2,2)
plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[model.labels_],
s=40)
plt.title('K Mean Classification')
plt.show()
km = rename(model.labels_)
print("\nWhat KMeans thought: \n", km)
print("Accuracy of KMeans is ",sm.accuracy_score(y, km))
print("Confusion Matrix for KMeans is \n",sm.confusion_matrix(y, km))
#The GaussianMixture scikit-learn class can be used to model this
problem
#and estimate the parameters of the distributions using the
expectation-maximization algorithm.
from sklearn import preprocessing
scaler = preprocessing.StandardScaler()
scaler.fit(X)
xsa = scaler.transform(X)
xs = pd.DataFrame(xsa, columns = X.columns)
from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=3)
gmm.fit(xs)
y_cluster_gmm = gmm.predict(xs)
plt.subplot(1, 2, 1)
plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[y_cluster_gmm],
s=40)
plt.title('GMM Classification')
plt.show()
em = rename(y_cluster_gmm)
print("\nWhat EM thought: \n", em)
print("Accuracy of EM is ",sm.accuracy_score(y, em))
print("Confusion Matrix for EM is \n", sm.confusion_matrix(y, em))

#program-8
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
iris_dataset=load_iris()
#display the iris dataset
print("\n IRIS FEATURES \ TARGET NAMES: \n ",
iris_dataset.target_names)
for i in range(len(iris_dataset.target_names)):
print("\n[{0}]:[{1}]".format(i,iris_dataset.target_names[i]))
# print("\n IRIS DATA :\n",iris_dataset["data"])
#split the data into training and testing data
X_train, X_test, y_train, y_test =
train_test_split(iris_dataset["data"], iris_dataset["target"],
random_state=0)
#train and fit the model
kn = KNeighborsClassifier(n_neighbors=5)
kn.fit(X_train, y_train)
for i in range(len(X_test)):
x = X_test[i]
x_new = np.array([x])
prediction = kn.predict(x_new)
print("\n Actual : {0} {1}, Predicted
:{2}{3}".format(y_test[i],iris_dataset["target_names"][y_test[i]],pre
diction,iris_dataset["target_names"][ prediction]))
print("\n TEST SCORE[ACCURACY]: {:.2f}\n".format(kn.score(X_test,
y_test)))

#program-9
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
def kernel(point,xmat, k):
 m,n = np.shape(xmat)
 weights = np.mat(np.eye((m))) # eye - identity matrix
 for j in range(m):
 diff = point - X[j]
 weights[j,j] = np.exp(diff*diff.T/(-2.0*k**2))
 return weights
def localWeight(point,xmat,ymat,k):
 wei = kernel(point,xmat,k)
 W = (X.T*(wei*X)).I*(X.T*(wei*ymat.T))
 return W
def localWeightRegression(xmat,ymat,k):
 m,n = np.shape(xmat)
 ypred = np.zeros(m)
 for i in range(m):
 ypred[i] = xmat[i]*localWeight(xmat[i],xmat,ymat,k)
 return ypred
def graphPlot(X,ypred):
 sortindex = X[:,1].argsort(0) #argsort - index of the smallest
 xsort = X[sortindex][:,0]
 fig = plt.figure()
 ax = fig.add_subplot(1,1,1)
 ax.scatter(bill,tip, color='green')
 ax.plot(xsort[:,1],ypred[sortindex], color = 'red', linewidth=5)
 plt.xlabel('Total bill')
 plt.ylabel('Tip')
 plt.show();
# load data points
data = pd.read_csv('E:\\data10.csv')
bill = np.array(data.total_bill) # We use only Bill amount and Tips data
tip = np.array(data.tip)
mbill = np.mat(bill) # .mat will convert nd array is converted in 2D array
mtip = np.mat(tip)
m= np.shape(mbill)[1]
one = np.mat(np.ones(m))
X = np.hstack((one.T,mbill.T)) # 244 rows, 2 cols
# increase k to get smooth curves
ypred = localWeightRegression(X,mtip,3)
graphPlot(X,ypred)
</pre>
